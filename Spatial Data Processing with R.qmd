---
title: Spatial Data Processing with R
author:
  - name: Dr. Jérôme Guélat
    affiliations:
      - name: Swiss Ornithological Institute
      - url: https://www.vogelwarte.ch
date: today
#affiliations: Swiss Ornithological Institute
#date-format: short
title-block-banner: true
format: html
page-layout: full
number-sections: true
toc: true
toc-depth: 3
toc-location: left
toc-title: ""
code-fold: false
#theme: sketchy
---

```{r}
#| echo: false
#| output: false
set.seed(555)
```

## Preparation

In order to run the code for this workshop, you'll need the following packages. You can use the following code to install them. All the required dependencies will be automatically installed.

::: callout-important
Please also update to the latest version of R, otherwise you may get packages that are not fully up-to-date.
:::

``` r
install.packages("sf")
install.packages("lwgeom")
install.packages("terra")
install.packages("spdep")
install.packages("tmap")
install.packages("mapview")
install.packages("mapsf")
```

You'll also need to download the following data that we'll use in some of the examples: <https://quarto.org>.

## Introduction

When we work with geographic data we need to decide how to model real world objects or concepts (buildings, forests, elevation, etc.) before we can use them in a computer with a GIS (Geographic Information System) software. GIS people mainly use 2 main data models: vector and raster. Other models exist, such as TINs, point clouds or meshes, but we won't cover them in this workshop.

Vector data is normally used for high precision data sets and can be represented as points, lines or polygons. Properties of the represented features are stored as attributes. The vector types you will use depends of course on your own data and on your analyses. For example: points could be appropriate for bird nests and sightings, lines for moving animals and linear structures (paths, rivers), and polygons for territories and land cover categories. Of course a river can also be modeled as a polygon if you're interested in its width (or you can also store its width as an attribute of the line).

::: callout-note
High precision doesn't necessarily mean high accuracy! For example the coordinates of some points could be stored in meters with 5 decimals even though the measurement error was 2 meters.

Most vector data formats include some possibility to store information about measurement errors but this is actually very rarely used.
:::

The best known format for storing vector data is the shapefile, an old and inefficient format developed by ESRI. Even though the shapefile format is still widely used, it has a lot of limitations and problems (listed on the following website: <http://switchfromshapefile.org>). Nowadays GIS specialists advise to replace it with better alternatives such as the **GeoPackage** format. Every modern GIS software can read and write GeoPackages and the format is also completely open-source. It is also published as a standard by the Open Geospatial Consortium (OGC) which makes it a future-proof alternative.

Raster data is basically an image divided into cells (pixels) of constant size, and each cell has an associated value. Satellite imagery, topographic maps and digital elevation models (DEM) are typical examples where the raster data model is appropriate. A raster data set can have several layers called bands, for example most aerial images have at least three bands: red, green and blue. In the raster data model, specific geographic features are aggregated to a given resolution to create a consistent data set, associated with a loss of precision. The resolution used to aggregate can have a large influence of some analyses and must be thought of carefully.

There exists thousands of different raster data formats. As of today I recommend using the **GeoTiff** format. It is widely used in the GIS world and every GIS software can read and write raster data in this format. Note that it is also possible to use the GeoPackage format to save raster data sets, however I would advise against using it since some GIS software won't be able to read these rasters.

::: callout-tip
Vector data: use the GeoPackage format

Raster data: use the GeoTiff format
:::

## Vector data

### Vector data model

The main vector types are points, lines and polygons (or a combination thereof) and the point is the base of all these types. For example a simple line consists of 2 connected points, similarly an ordered sequence of connected points will represent a more complex line (often called a polyline). A simple polygon will be modeled as an external ring, which is a special type of polyline where the first and last points are identical. In the case of lines and polygons we often speak of vertices to describe these points. Things can be a bit more complex, for example a polygon could have a hole which is modeled as an internal ring.

The **Simple Feature** standard ([full documentation](https://portal.ogc.org/files/?artifact_id=25355)) was developed to be sure that we all speak the same language when describing vector elements. The specification describes 18 geometry types, but don't worry only 7 of them will be useful for us. The following figure shows these 7 types (source: Lovelace *et al.*, 2019):

![](figures/sf-classes.png)

A feature represents a geographic entity modeled by one of these types. For example a building would be a single feature of type POLYGON, while the whole Hawaii archipelago would be a single feature of type MULTIPOLYGON (but you could of course also model each island as type POLYGON). A single feature using the MULTI\* types can have multiple elements but this is not mandatory. Most of the time we will use the default 2D version of these types. However it is possible to include additional numeric values such as the height of each point (Z values) or some kind of measurement error (M values). Note that most GIS software will ignore these values for the vast majority of spatial analyses.

::: callout-important
The feature type is usually defined for the whole vector data set, and not per feature (well actually `sf` lets you do that but this will brings you all sorts of troubles). For example, if you know that your data set will contain POLYGON and MULTIPOLYGON features, then you will have to use the MULTIPOLYGON type for all of them.
:::

In most GIS softwares (including R), simple features are internally encoded using the well-known binary (WKB) or well-known text (WKT) standards. As the name mentions, WKB is a binary format and hence not easily readable by normal humans. The WKT format is encoding exactly the same information as WKB, but in a more human friendly way. Here are some examples of WKT-encoded features (check the [Wikipedia page](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) if you need more):

-   a point: POINT (10 5)
-   a linestring made of 3 points: LINESTRING (1 1, 2 4, 5 10)
-   a polygon (without a hole): POLYGON ((10 5, 10 9, 5 8, 4 2, 10 5))
-   a multilinestring: MULTILINESTRING ((1 1, 2 4, 5 10), (2 2, 5 2))

The geometry is of course essential in order to have a spatial information but the vector data model also allows storing non-spatial attributes (often called *attribute table*) for each feature. As we will see, these tables are stored as data frames in R and each column will store some property of the related feature (identification number, name, etc.). Each row relates to a single spatial feature (which can consist of several geometries if its type is MULTI\*). The following figure shows some examples (source: Tennekes & Nowosad, 2021):

![](figures/vector-data-model-1.png){width="600"}

### A first look at vector data in R

Let's have a look at how R stores a vector data set. The main classes and methods needed to work with spatial vector data are defined in the `sf` package (Pebesma, 2018). We will also load the `tmap` package (Tennekes, 2018) to have access to some spatial data sets.

```{r}
library(tmap)
library(sf)
```

When you first load the `sf` package, it will provide you with version information about some important open-source GIS libraries it uses. In a few rare cases, some functions will only be available if you use recent version of these libraries. If you use `sf` on Windows or Mac and install it from CRAN, they will be included inside the `sf` package and there's no easy way to update them. These libraries are used in almost all open-source GIS software and even in some commercial ones. GDAL takes care of reading and writing your GIS files and can read 99.9% of all the existing GIS formats; GEOS is a Euclidean planar geometry engine and is used for all the common GIS analyses (intersection, union, buffer, etc.); PROJ is responsible for all the coordinate reference systems operations. The s2 library is a spherical geometry engine which is active by default for computations when using unprojected data.

::: callout-important
## A bit of history

The availability of the `sf` package was huge change in the small world of "R-Spatial". In the old days we used a combination of several packages to process GIS vector data in R. The spatial classes (and some functions) were defined in the `sp` package, the import/export of data was managed by the `rgdal` package, and the geometric operations were available in the `rgeos` package. You'll find a lot of code using these packages on the internet. Please refrain from using them since they'll only be maintained until end of 2023 and will then probably be removed from CRAN. Moreover the `sf` package is definitely more powerful and much faster.
:::

We will now load the `World` vector data set inside the `tmap` package and have a look at its structure.

```{r}
data(World)
class(World)
names(World)
World
```

We see the `World` object is stored as a data frame with an additional geometry column (note that the name of the geometry column doesn't need to be 'geometry'). The content of the geometry column is displayed using the WKT standard. R is also giving us more information, like the coordinate reference system used (more on that later) and the number of dimensions (i.e. XY, XYZ or XYZM).

::: {.callout-note collapse="true" icon="false"}
## Question: why is the MULTIPOLYGON type appropriate?

Don't forget that each feature (i.e. each row of the data frame) represents a country, and some countries are made up of several distinct pieces of land (e.g., islands, exclaves). That's why we need the MULTIPOLYGON type. And since the type apply to the whole data set, even countries with a single geometry (like Switzerland) will need to be MULTIPOLYGONS.
:::

It is also easy to plot the data using the usual command.

```{r}
#| warning: false
plot(World)
```

By default R will take the first 9 attributes of the `sf` object and plot them using the available geometries. Since these objects inherit from the data base class, you can use all the typical data frame functions such as `summary`, `head`, `merge`, `rbind`, etc. Subsetting is also possible using the standard `[]` operators. Therefore you can use the following code if you only want to plot the well-being index, for the whole world, only for countries with a high index, or just for Australia.

```{r}
plot(World[,"well_being"])
plot(World[which(World$well_being > 6),"well_being"])
plot(World[which(World$name == "Australia"),"well_being"])
```

Note that the color scale was adapted depending on the available values in the filtered data set. If you only need the geometries without any attributes, then you can use the `st_geometry()` function.

```{r}
plot(st_geometry(World))
```

::: {.callout-note icon="false"}
## Exercise (5 minutes)

Play a little bit with the `World` data set, try different functions that you would normally use with a data frame. Import the `redlist` data set in the file `red_list_index.csv` (source: <https://stats.oecd.org>) and join it to the `World` data frame to add new attributes. Plot a world map using one of the new attributes.

```{r}
#| eval: false
#| code-fold: true
redlist <- read.csv("red_list_index.csv")
world2 <- merge(World, redlist, by.x = "iso_a3", by.y = "code")
plot(world2[,"index_2020"])
```
:::

### Structure of `sf` objects

Most of the time you won't need to create your own `sf` objects from scratch since you'll import some existing GIS data. But if you need to, there are special functions to help you. This is also a good way to get a better understanding of the structure of `sf` objects. The standard process is shown in the following figure (source: Lovelace *et al.*, 2019):

![](figures/02-sfdiagram.png){width="800"}

You first need to create each feature geometry using some constructor functions. Each of these features will be of class `sfg` (simple feature geometry). Then you collect all these geometries in a list using the `st_sfc()` function. You get a new object of class `sfc` (simple feature list-column). After that you combine the newly created simple feature list-column with the attributes (stored as a data frame, or a tibble) using the `st_sf()` function in order to get an `sf` object.

Since this is rather abstract, let's look at a simple example. Imagine we want to create a point data set containing three bird observations, and each observation will have the following attributes: species and sex. We start by creating our point geometries:

```{r}
pt1 <- st_point(c(2657000, 1219000))
pt2 <- st_point(c(2658000, 1218000))
pt3 <- st_point(c(2659000, 1217000))
```

Let's have a look at what we've just created:

```{r}
pt1
class(pt1)
typeof(pt1)
str(pt1)
```

Our first object is a 2D point (otherwise we would see XYZ or XYZM) of class `sfg`. If we look a bit more into the details of the structure, we see that it is actually stored as vector of type `double` (with length 2).

Now we need to collect our points inside an `sfc` object. This is simply a list of `sfg` objects with an associated coordinate reference system (CRS). Since we collected our data in Switzerland, we will use the standard Swiss coordinate reference system. As we will see later, most coordinate reference systems are identified by a specific number.

```{r}
pts <- st_sfc(pt1, pt2, pt3, crs = 2056)
```

Let's have a look at our new creation:

```{r}
pts
class(pts)
typeof(pts)
str(pts)
```

This confirms that our `sfc` object is actually a list, and this object will be the geometry column of the soon to be created `sf` object. Since our object is a list, it is easy to extract individual elements if needed:

```{r}
# Extract the second item of the list
pts[[2]]
class(pts[[2]])
```

The feature geometries are defined and stored in an `sfc` object, now we just need to define the attributes of each feature. We store them in a data frame using the same order as the geometries.

```{r}
pts_data <- data.frame(species = c("wallcreeper", "alpine chough", "kingfisher"),
                       sex = c("male", "female", "female"))
pts_data
```

And as a last step we combine the feature geometries with the related attributes using the `st_sf()` function. We now have a typical GIS data set stored as an `sf` object.

```{r}
pts_sf <- st_sf(pts_data, geometry = pts)
pts_sf
```

Since everything is stored as lists, it is again easy to access individual elements of the `sf` object:

```{r}
# Extract the 3rd geometry
pts_sf$geometry[[3]]
```

We process similarly to create other geometry types from scratch, the only difference is that we now need matrices to store the vertices of the lines and polygons instead of a simple vector, and for multilinestrings, (multi-)polygons and geometry collections, we need more lists to encapsulate everything. If you're not sure how to create geometries, the `sf` documentation provides examples for all the geometry types. Look for the following functions: `st_point()`, `st_linestring()`, `st_polygon()`, `st_multipoint()`, `st_multilinestring()`, `st_multipolygon()`, `st_geometrycollection()`. Here's a more complex example showing how to create a multipolygon (including one geometry with a hole) inside a `sfg` object. The next steps (collecting geometries in a `sfc` object, adding attributes and store as a `sf` object) are exactly the same as before.

```{r}
# rbind creates matrices and makes the coding easier
pol1_border <- rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))
pol1_hole <- rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))
pol1 <- list(pol1_border, pol1_hole)
pol2 <- list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))
multipolygon_list <- list(pol1, pol2)
multipol <- st_multipolygon(multipolygon_list)
multipol
plot(multipol, col = "navy")
```

::: {.callout-note icon="false"}
## Exercise (5 minutes)

Try to build your own `sfc` and `sf` objects.
:::

## Raster data

As we saw above, a raster data set is basically an image, which is the same as a grid of pixels. Most raster data sets you will encounter will have a constant pixel size (also called resolution) and we will only focus on these during this workshop. However don't forget that other kind of grids, for example sheared or curvilinear, also exist. This is sometimes needed depending on the coordinate reference system used to store the data, or can be caused by some reprojections.

Rasters are perfect for storing continuous values contained in a large area (called the extent of the raster). Digital elevation models are a typical example of such data, each cell is used to store an elevation value. You will also find rasters containing discrete values, these are often used to store landcover or landuse data sets. Note that, unlike vector data, it is impossible to store overlapping features in the same data set. We saw that vector data sets can store multiple attributes for a single feature. We can use a similar technique for raster data sets with the help of raster bands. You can think of raster bands as different layers of the same grid, each layer containing a different information. This is mainly use for spectral data, for example the red, green and blue intensity values in an aerial picture; satellite imagery will have even more bands depending on the number of sensors. Multiband rasters are also often used to store environment variables that change through time (e.g. a temperature raster, with one band per day). Such rasters are often called datacubes.

![](figures/raster_types.jpg)

Performing computations on raster data sets is usually very efficient and faster than using vector data, this is due to the fact that rasters are stored in some kind of matrix formats with some extra information (such as the coordinate reference system and the origin of the raster). It this thus possible to use highly efficient linear algebra libraries. The mathematical operations performed on raster cells are called map algebra.

For this workshop we will use the `terra` package to work with raster data. This package has everything needed to import, analyses, visualize and export raster data sets. Like the `sf` package, it is also using the GDAL library for the import/export operations, which means it can open almost every raster data format. Unlike the `sf` package, `terra` will not import the full data sets in memory but only create a pointer to the data and then read smaller blocks of data successively. This allows working with very large rasters with a relatively small memory footprint. The amount of functions available in the `terra` package is similar to typical GIS software.

::: callout-important
## Rasters and R workspaces

Since `terra` only stores a pointer to the raster data set, this means the actual data set won't be included if you save your session in an R workspace (.Rdata files). If you really want to include it in your workspace, you can use the `wrap()` function. Note that this is also needed if you want to pass raster data over a connection that serializes, e.g. to a computer cluster.
:::

There is another famous R package to process raster data, the `stars` package. It is especially useful if you need to work with "irregular" rasters (sheared, curvilinear, etc.) or with complex datacubes. It is also tidyverse-friendly and the syntax is closed to the one use in `sf`. However the number of available functions is (still) much lower than in `terra`. If you need to use both packages, it is fortunately easy to convert raster objects from `terra` to `stars` (using the function `st_as_stars()`), and the other way round (using the function `rast()`).

::: callout-note
## A bit of history

A revolution happened in 2010 in the small world of R-Spatial when the `raster` package was released. People were able to perform analyses using raster data sets in R instead of using a standard GIS software. The package has been maintained during many years, and many functions were added. However its developer decided to create a new package from scratch in order to improve speed and memory efficiency, the `terra` package was born. You will often find a lot of R code using the `raster` package on the web. Fortunately it is quite easy to adapt code written for the `raster` package to `terra`. The functions have similar names (sometimes even identical) and everything that was available in `raster` should also be available in `terra`. Actually the recent versions of `raster` even use `terra` in the background instead of the original `raster` code.
:::

## Coordinate reference systems

The majority of normal people will get scared if there's some problem to solve involving coordinate reference systems or projections. That's why I will keep this part really short and only show you the stuff you will need to perform standard GIS analyses with R. If you want to read more about this (extremely interesting) topic, I invite to read the following book chapters: <https://geocompr.robinlovelace.net/spatial-class.html#crs-intro> and <https://r-tmap.github.io/tmap-book/geodata.html#crs>.

There is a famous expression saying "spatial is special"... One of the main reasons is that such data will have an associated location and you thus need to a reference frame to describe this location. This reference frame is called a coordinate reference system (CRS) in the GIS world. CRSs can be either geographic or projected.

::: callout-important
When you're working with GIS data, you should always know the CRS you're using. Otherwise coordinates are just numbers without a reference frame. When you share GIS data, make sure the CRS is always defined in the data set or documented in some other way. The CRS of a vector data set can be queried with the `st_crs()` function, for a terra object you should use the `crs()` function.
:::

A geographic CRS will identify locations on a spheroid or an ellipsoid using 2 values: latitude and longitude. The shape of the Earth is actually a geoid, but it is too complex to perform computations and thus one has to use approximations. The spheroid is making the assumptions that the Earth is a sphere, while the ellipsoid is a better approximation accounting for the fact that our planet is a bit compressed. Geographic coordinate systems are not using a projection! All the computations (distances, buffers, etc.) have to happen on the spheroid/ellipsoid which makes things more complex. It is easy to make mistakes when working with geographic CRSs, and even smart people fell in this trap (e.g. <https://georeferenced.wordpress.com/2014/05/22/worldmapblunders>).

Projected CRSs are based on geographic CRSs but include an additional projection step on a flat surface. When using a projected CRS, locations are described using Cartesian coordinates called easting and northing (x and y). Projecting a spherical or ellipsoidal surface on a plane will cause deformations. These will affect four properties of the surface: areas, distances, shapes and directions. A projected CRS can preserve only one or two of these properties. There exists a ton of different projections and all of them make different compromises, some are even totally useless (check this beautiful xkcd comic: <https://xkcd.com/977>). Choosing a projection can be challenging, especially if your data covers a very large area. The following websites allow you to visualize the main projection types: <https://www.geo-projections.com> and <https://map-projections.net/singleview.php>. The second website also provides a nice tool to visualize distortions called a Tissot indicatrix. Fortunately, if your data is within a "smallish" area, it is relatively easy to find a good projected CRS that has only minimal distortions. Almost every country has its own recommended projected CRS (or CRSs), and if your data covers several countries, you can use some UTM (Universal Transverse Mercator) coordinate systems.

::: callout-tip
It is almost always easier to work with a projected CRS, except if your data is really global (or covering a really large area, like a continent). Moreover, most GIS software will (still) make the assumption that you're data is on a flat plane, even if you're working with a geographic CRS. The `sf` package is kind of an exception since it will actually perform calculations on a spheroid if you use a geographic CRS, thanks to the s2 library.
:::

::: callout-important
The CRS used by almost all mapping websites (OpenStreetMap, Google Maps, etc.) should never be used for any analysis. It is a slightly modified version of the Mercator projection called Web Mercator or Pseudo-Mercator. It has some advantages allowing good visualization speed, but the distortions are massive. Check the following website: <https://www.thetruesize.com>.
:::

With so many CRSs available, we need a way to classify them. That's what the EPSG (European Petroleum Survey Group) started doing a few years ago. They collected and documented most available CRSs in a data set which is now called the EPSG Geodetic Parameter Dataset (<https://epsg.org/home.html>). In this data set, every CRS has a unique identification number that can be used in a GIS software instead of writing the full definition of the CRS. The best available transformations between CRSs are also defined. Sadly this data set is still missing a few interesting CRSs and was thus completed by other companies such as ESRI. This is the reason why you'll sometimes see ESRI codes instead of EPSG for some CRSs. The following CRSs are the most important for us:

| Code         | Name                  | Description                                                                            |
|------------------|------------------|------------------------------------|
| 2056 (EPSG)  | CH1903+/LV95          | Projected CRS currently used in Switzerland                                            |
| 21781 (EPSG) | CH1903/LV03           | Former projected CRS used in Switzerland, you will still find data sets using this one |
| 4326 (EPSG)  | WGS84                 | Geographic CRS used for most global data sets, and by GPS devices                      |
| 3857 (EPSG)  | Pseudo-Mercator       | Projected CRS used by online maps                                                      |
| 8857 (EPSG)  | Equal Earth Greenwich | Nice equal-area projection for world maps                                              |
| 54030 (ESRI) | Robinson              | Aesthetically pleasing projection for world maps                                       |

::: callout-important
## Proj4strings

When looking for examples on the web, you will often find code using what is called a proj4string to define a CRS or to reproject data. For example the proj4string for the current Swiss CRS looks like this: `+proj=somerc +lat_0=46.9524055555556 +lon_0=7.43958333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs +type=crs`. This was the standard way of describing CRSs until a few years ago. You should NOT use these strings, instead always use the EPSG number to be on the safe side. Otherwise you may get small to medium position errors when reprojecting your data.
:::

::: {.callout-note icon="false"}
## Exercise (5 minutes)

You can easily inspect the way CRSs are stored in modern GIS software. For example if you want to inspect the current Swiss CRS, try `st_crs(2056)`. You will get the full description of the CRS. Try to understand some of the elements. If you use EPSG codes, you can simply enter the code as an integer, if it's an ESRI code you should use `st_crs("ESRI:xxxxx")` where `xxxxx` is the ESRI code.
:::

## Tips and tricks

### Reading and writing vector data

We had a look at the gory details of the internal structure `sf` objects. However most of the time you will not create such objects on your own but rather rely on the `sf` package to create the right structure when you import existing GIS data. The `sf` package is using the GDAL (Geospatial Data Abstraction Library) library to read and write GIS files, and this means you will be able to import almost all existing GIS vector formats. Note that the vector part of the GDAL library is often called OGR. Sometimes you will not get a standard GIS data set but a simple CSV (or Excel) file containing coordinates and related attributes. We will first see how to import these different data types in order to use them with the `sf` package, and then how to export `sf` objects in a standard GIS format.

#### Importing a GeoPackage

The GeoPackage format is the best available open format to store vector data. It is based on the SQLite database format which is probably one of the most used file-based database nowadays. You can think of it as a special folder containing one or several GIS data sets. Since we normally don't know in advance if a GeoPackage contains one or more data sets, we first have to inspect it.

```{r}
st_layers("data/geodata.gpkg")
```

You should not always trust the reported number of features. Some GIS format such as the GeoPackage report this number, some don't. If the GeoPackage was produced by a software that doesn't properly implement the standard, the reported number of features can be wrong (but this shouldn't have any other bad consequence). If you want to be sure to get the correct number, you can use the `do_count = TRUE` argument of the `st_layers()` function, but this will be slower.

To read the data, you use the `st_read()` function, the first argument is the path of the GeoPackage, and the second argument is the layer you want to import. The function will return a `sf` object. By default you'll get some information about the data being imported. If you don't need them, you can use the argument `quiet = TRUE`.

```{r}
muni <- st_read("data/geodata.gpkg", "municipalities")
streets <- st_read("data/geodata.gpkg", "streets", quiet = TRUE)
muni
```

We see some basic information about the data set, and the first features are shown as well with all the attributes. The municipalities data set in an extract of the swissBOUNDARIES3D provided by Swisstopo, the streets data set is an extract of the swissTLM3D also provided by Swisstopo.

#### Importing a Shapefile

If you really need to import a Shapefile, you can use the same function. Since Shapefiles cannot contain more than one data set, we only need to provide the first argument of the function. A Shapefile consists of several file with different extensions (.shp, .shx, etc.), we use the .shp extension by default when importing.

```{r}
muni2 <- st_read("data/municipalities.shp", quiet = TRUE)
muni2
```

This is actually the same data set as the one in the GeoPackage, however we seen that `sf` is now using polygons instead of multipolygons. This is caused by the fact that the Shapefile format does not distinguish properly between the two types. You can thus have a combination of polygons and multipolygons in the same data set.

#### Importing from a PostGIS database

PostGIS is a famous open-source extension for the PostgreSQL database management system. It allows storing all kind of GIS data inside a database and perform hundreds of typical GIS analyses. The Swiss Ornithological Institute is using PostGIS to store almost all its bird data and a lot of other GIS data sets. If you have a laptop provided by the institute and you have access to the institute internal network (via cable or private Wi-Fi, not the public one), you should be able to run the following code.

First we need to load the `RPostgres` package which provides function to access PostgreSQL databases (and hence PostGIS, too). There is another package providing similar functionality called `RPostgreSQL`, but in my opinion the `RPostgres` is better maintained and I experienced less problems.

After storing all the connection details in some variables, we can finally create a connection to the database using the `dbConnect()` function.

```{r}
#| eval: false
library(RPostgres)

# Login data
user <- "gast_vowa"
password <- scan("/Users/jguelat/Desktop/pass.txt", what = "character", quiet = TRUE)
host <- "dbspatialdata1"
database <- "research"

# Connection to the database
dbcon <- dbConnect(Postgres(), dbname = database, host = host, user = user, password = password)
```

After that we need to import the data with a query, using again the `st_read()` function. Note that the first argument of the function must be the database connection object. The first possibility consists of importing the whole layer (called table in the database lingo) with all its attributes. This is what we do for the `cantons1` data set. We need to use the `Id()` function to specify the location of the table inside the database. In a PostgreSQL database, a schema is a bit like a folder where we store tables, this allows us to implement some structure inside the database. In our case the table `cantonal_boundaries_ch` is stored inside the schema `perimeter`. Note that we don't need the `Id()` function if the table are stored in the `public` schema.

We can also specify a SQL query to import the data, like for the `cantons2` data set. Using this kind of query, we are fully flexible. We can for example specify the attributes we want to import, specific data filters, we can even join different tables together (by attributes or even spatially). Once again we have to specify the schema, but this is done a bit differently.

Once we have our `sf` objects, we still need to disconnect the database. The cantonal_boundaries_ch data set contains all the cantonal boundaries in Switzerland. The data is provided by Swisstopo (swissBOUNDARIES3D).

```{r}
#| eval: false
# Load cantonal boundaries
cantons1 <- st_read(dbcon, layer = Id(schema = "perimeter", table = "cantonal_boundaries_ch"))
cantons2 <- st_read(dbcon, query = "SELECT id, name, geom
                                    FROM perimeter.cantonal_boundaries_ch
                                    WHERE name = 'Fribourg'")

# Disconnect database
dbDisconnect(dbcon)

# Show sf objects
cantons1
cantons2
```

#### Importing a CSV file with coordinates

If you have a table containing coordinates of point data (e.g. sites or bird sightings), you should use the `st_as_sf()` function. The first argument should be the dataframe containing the data, and you also need to specify the names of the columns containing the geographic coordinates, and the CRS used. This data set was extracted from the bird sightings database of the Swiss Ornithological Institute.

```{r}
obs <- read.csv("data/observations.csv")
head(obs)

obs <- st_as_sf(obs, coords = c("x", "y"), crs = 2056)
obs
```

#### Exporting to GeoPackage

To export vector data, you need to use the `st_write()` function. Like the `st_read()` function, it uses the GDAL library, so you'll be able to export in many different formats. You can specify the format explicitely, otherwise `sf` will try to guess it based on the file extension. For a GeoPackage, you need to specify the name of the GeoPackage first (it will be automatically created if it doesn't exist) and the name of the data set that will be stored inside the GeoPackage. If you specify a GeoPackage that already exists, the data set will be added to it as a new table.

```{r}
#| echo: false
#| output: false
if(file.exists("export/birds.gpkg")) {file.remove("export/birds.gpkg")}
```

```{r}
st_write(obs, "export/birds.gpkg", "observations")

obs2 <- obs[1:10,]
st_write(obs2, "export/birds.gpkg", "observations2", quiet = TRUE)
st_layers("export/birds.gpkg")
```

If you want to delete a data set, you can use the `st_delete()` function. Think twice before doing it, there will be no warning!

```{r}
st_delete("export/birds.gpkg", "observations2")
```

#### Exporting to Shapefile

Please don't!

### Reading and writing raster data

As we saw earlier, we need another package to import raster data sets. The `rast()` function from the `terra` package is what we need. It doesn't matter if our raster is continuous, discrete, or contains several bands, the `rast()` function will create the correct `terra` object. However don't forget that it's only a pointer to the data set, the full raster is not imported into memory. Calling the object will display some basic information about the data set.

The digital elevation model is an extract of the DHM25 data set provided by Swisstopo, the orthophoto is an extract of the SWISSIMAGE data set also provided by Swisstopo.

```{r}
#| echo: false
#| output: false
if(file.exists("export/dem.csv")) {file.remove("export/dem.csv")}
if(file.exists("export/dem1.tif")) {file.remove("export/dem1.tif")}
if(file.exists("export/dem2.tif")) {file.remove("export/dem2.tif")}
```

```{r}
library(terra)
elev <- rast("data/dem.tif")
elev
plot(elev)
ortho <- rast("data/sempach_ortho.tif")
ortho
plot(ortho)
```

Note that the orthophoto was plotted in a different way (no axes, no legend). The `terra` package automatically detected that the raster had 3 bands and made the assumption that the bands corresponded to red, green and blue intensity values. If this assumption is not correct, you can use the `plotRGB()` function to get the desired plot.

Rasters are nice objects to work with but sometimes it's nice with more familiar R objects. You can easily convert `SpatRaster` objects to dataframes containing the geographic coordinates of all pixels and the related pixel values.

```{r}
elev_df <- as.data.frame(elev, xy = TRUE)
head(elev_df)
write.csv(elev_df, "export/dem.csv")
```

Similarly, if you have a dataframe containing geographic coordinates located on a regular grid and associated values, you can easily convert it to a `terra` object with the help of the `rast()` function, by adding the argument `type = "xyz"`. This is often useful if you want to plot the predictions of a statistical model on a map, for example the distribution of a species. Here we add a new column to some dataframe filled with random values and we convert it to a 2-band raster.

```{r}
elev_df$rand <- rnorm(nrow(elev_df))

elev2 <- rast(elev_df, type = "xyz")
elev2
plot(elev2)
```

Exporting raster data set is really easy thanks to the `writeRaster()` function. The format will be automatically recognized based on the filename extension. As we saw in the introduction, the GeoTiff format is almost always the format we should use.

```{r}
writeRaster(elev, "export/dem1.tif")
writeRaster(elev2, "export/dem2.tif")
```

Remember that the first raster was a single band raster and the second one had two bands. What is happening when we visualize the latter with QGIS? It looks like QGIS is combining the bands like an RGB raster. If you want to only export a single band, you can subset the raster data set using `[[]]`, either using the band number or name.

```{r}
writeRaster(elev2[[1]], "export/dem2.tif", overwrite = TRUE)
writeRaster(elev2[["dem"]], "export/dem2.tif", overwrite = TRUE)
```

### Basic geometric computations

```{r}
st_area(muni)
head(st_length(streets))
```

```{r}
library(lwgeom)
st_perimeter(muni)
```

```{r}
muni_centroid <- st_centroid(muni)
st_coordinates(muni_centroid)
head(st_coordinates(muni))
```

Centroid outside, st_point_on_surface()

```{r}
temp <- st_read("data/geodata.gpkg", "invalid", quiet = TRUE)
plot(temp, col = 1:nrow(temp))
st_area(temp)
```

Why is this happening? We need to talk a bit about geometric validity...

### Geometric validity

```{r}
temp
st_is_valid(temp)
st_is_valid(temp, reason = TRUE)
```

```{r}
temp_valid <- st_make_valid(temp)
temp_valid
st_geometry_type(temp_valid)
st_geometry_type(temp_valid, by_geometry = FALSE)
```

### Vector type casting

```{r}
temp_multipoly <- st_cast(temp_valid, to = "MULTIPOLYGON")
st_as_text(st_geometry(temp_valid))
st_as_text(st_geometry(temp_multipoly))
st_geometry_type(temp_multipoly, by_geometry = FALSE)
```

```{r}
temp_poly <- st_cast(temp_multipoly, to = "POLYGON")
temp_multiline <- st_cast(temp_multipoly, to = "MULTILINESTRING")
temp_line <- st_cast(temp_multiline, to = "LINESTRING")
temp_multipts <- st_cast(temp_multipoly, to = "MULTIPOINT")
temp_pts <- st_cast(temp_multipoly, to = "POINT")

temp_poly
temp_multiline
temp_line
temp_multipts
temp_pts

par(mfrow=c(2, 3))
plot(st_geometry(temp_multipoly), col = 1:nrow(temp_multipoly))
plot(st_geometry(temp_poly), col = 1:nrow(temp_poly))
plot(st_geometry(temp_multiline), col = 1:nrow(temp_multiline))
plot(st_geometry(temp_line), col = 1:nrow(temp_line))
plot(st_geometry(temp_multipts), col = 1:nrow(temp_multipts), pch = 16)
plot(st_geometry(temp_pts), col = 1:nrow(temp_pts), pch = 16)
```

```{r}
sempach_multiline <- st_cast(muni[which(muni$name == "Sempach"),], to = "MULTILINESTRING")
plot(st_geometry(sempach_multiline), col = "navy")
sempach_poly <- st_cast(sempach_multiline, to = "POLYGON")
plot(st_geometry(sempach_poly), col = "navy")
```

::: {.callout-note icon="false"}
## Exercise (5 minutes)

Create a new object containing only the polygon for Sursee, check its validity and compute its perimeter using the `st_length()` function, compare with the result of the `st_perimeter()` function.

```{r}
#| eval: false
#| code-fold: true
sursee_poly <- muni[which(muni$name == "Sursee"),]
sursee_multiline <- st_cast(sursee_poly, to = "MULTILINESTRING")
st_length(sursee_multiline)
st_perimeter(sursee_poly)
```
:::

### Spatial predicates

### Distance operations

```{r}
world_centroid <- st_centroid(World)
sf_use_s2(FALSE)
pt1 <- world_centroid[world_centroid$name == "Switzerland",]
pt2 <- world_centroid[world_centroid$name == "Australia",]
(d1 <- st_distance(pt1, pt2))
sf_use_s2(TRUE)
(d2 <- st_distance(pt1, pt2))
d2 - d1
```

```{r}
#| echo: false

library(sp)
library(geosphere)
tmp1 <- as(pt1, "Spatial")
tmp2 <- as(pt2, "Spatial")
gcLine <- greatCircle(tmp1, tmp2, sp = TRUE)
gcLine <- st_as_sf(gcLine)

world_proj <- st_transform(World, "ESRI:54030")
pt1_proj <- st_transform(pt1, "ESRI:54030")
pt2_proj <- st_transform(pt2, "ESRI:54030")
gcLine_proj <- st_transform(gcLine, "ESRI:54030")

pt3 <- st_snap(pt1_proj, gcLine_proj, 10000)
pt4 <- st_snap(pt2_proj, gcLine_proj, 100000)
part <- st_collection_extract(st_split(gcLine_proj$geometry, pt3$geometry), "LINESTRING")[2]
part <- st_collection_extract(st_split(part, pt4$geometry), "LINESTRING")[1]

extent <- world_proj[which(world_proj$name == "Iceland" | world_proj$name == "Norway" | world_proj$name == "Australia"),]
plot(st_geometry(world_proj), extent = extent, col = "grey90")
plot(part, add = TRUE, lwd = 2, col = "red")
plot(st_geometry(pt1_proj), add = TRUE, pch = 16, col = "red")
plot(st_geometry(pt2_proj), add = TRUE, pch = 16, col = "red")
```

### Aggregate by attributes

```{r}
aggregate(pop_est ~ continent, FUN = sum, data = World, na.rm = TRUE)
world_agg <- aggregate(World["pop_est"], list(World$continent), FUN = sum, na.rm = TRUE)
plot(world_agg[,"pop_est"])

muni_agg <- st_union(muni)
```

### Great circle distances

st_distance, great circle distance calculations use by default spherical distances (s2_distance or s2_distance_matrix); if sf_use_s2() is FALSE, ellipsoidal distances are computed using st_geod_distance which uses function geod_inverse from GeographicLib (part of PROJ); see Karney, Charles FF, 2013, Algorithms for geodesics, Journal of Geodesy 87(1), 43--55

## Making maps

## More help

sf vignettes

geocomp with R

tmap book

## References

Appelhans T., Detsch F., Reudenbach C., Woellauer S. (2022). mapview: Interactive Viewing of Spatial Data in R. R package version 2.11.0

Bivand R. (2022) R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data Geographical Analysis, 54(3), 488-518

Giraud T. (2022). mapsf: Thematic Cartography. R package version 0.6.1

Hijmans R. (2022). terra: Spatial Data Analysis. R package version 1.6-47

Pebesma E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439-446

Pebesma E., Bivand R. 2022

Tennekes M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1-39

Tennekes M., Nowosad J. (2021). Elegant and informative maps with tmap. https://r-tmap.github.io/tmap-book/index.html
